"""
å‡ºæ°´æµŠåº¦é¢„æµ‹ - xPatch è®­ç»ƒè„šæœ¬

ç”¨æ³•:
    # è®­ç»ƒå•ä¸ªæ± å­
    python train.py --pool 1 --epochs 100

    # è®­ç»ƒæ‰€æœ‰æ± å­
    python train.py --all --epochs 100

    # CPU å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆå°‘é‡æ•°æ® + å°‘é‡ epochï¼‰
    python train.py --pool 1 --epochs 3 --test-mode

    # ç›´æ¥é¢„æµ‹æ¨¡å¼ â€” é¢„æµ‹ç»å¯¹å€¼
    python train.py --pool 1 --epochs 100 --no-diff
"""

import os
import sys
import argparse
import time
import json
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from dataclasses import dataclass

from dataset import create_datasets, TURB_CHUSHUI_IDX
from models.xPatch import Model


# ============================================================
# é…ç½®
# ============================================================

@dataclass
class ModelConfig:
    """xPatch æ¨¡å‹é…ç½®"""
    seq_len: int = 30       # è¾“å…¥åºåˆ—é•¿åº¦
    pred_len: int = 6       # é¢„æµ‹é•¿åº¦
    enc_in: int = 6         # è¾“å…¥é€šé“æ•°
    patch_len: int = 6      # Patch é•¿åº¦
    stride: int = 3         # Patch æ­¥é•¿
    padding_patch: str = 'end'  # Patch å¡«å……æ–¹å¼
    d_model: int = 64       # æ¨¡å‹éšè—ç»´åº¦ï¼ˆunused in new xPatch, kept for compatï¼‰
    revin: bool = True      # æ˜¯å¦ä½¿ç”¨ RevIN
    ma_type: str = 'ema'    # ç§»åŠ¨å¹³å‡ç±»å‹: 'ema', 'dema', 'reg'ï¼ˆä¸åˆ†è§£ï¼‰
    alpha: float = 0.5      # EMA å¹³æ»‘å› å­
    beta: float = 0.5       # DEMA å¹³æ»‘å› å­ï¼ˆä»… dema æ¨¡å¼ç”¨ï¼‰


@dataclass
class TrainConfig:
    """è®­ç»ƒé…ç½®"""
    csv_path: str = ''
    pool_id: int = 1
    epochs: int = 100
    batch_size: int = 64
    lr: float = 1e-3
    weight_decay: float = 1e-4
    patience: int = 10      # æ—©åœè€å¿ƒå€¼
    use_diff: bool = True   # å·®åˆ†ç›®æ ‡
    output_dir: str = 'output'
    test_mode: bool = False  # CPU å¿«é€Ÿæµ‹è¯•æ¨¡å¼


# ============================================================
# è®­ç»ƒé€»è¾‘
# ============================================================

def train_one_pool(train_cfg: TrainConfig, model_cfg: ModelConfig, device: torch.device):
    """è®­ç»ƒå•ä¸ªæ± å­çš„ xPatch æ¨¡å‹"""
    pool_id = train_cfg.pool_id
    print(f"\n{'='*60}")
    print(f"  è®­ç»ƒæ± å­ {pool_id}  |  è®¾å¤‡: {device}")
    print(f"{'='*60}")

    # è¾“å‡ºç›®å½•
    pool_dir = os.path.join(train_cfg.output_dir, f'pool_{pool_id}')
    os.makedirs(pool_dir, exist_ok=True)
    scaler_path = os.path.join(pool_dir, 'scaler.pkl')

    # åˆ›å»ºæ•°æ®é›†
    print("[1/4] åŠ è½½æ•°æ®...")
    train_ds, val_ds, test_ds, scaler, feature_names = create_datasets(
        csv_path=train_cfg.csv_path,
        pool_id=pool_id,
        seq_len=model_cfg.seq_len,
        pred_len=model_cfg.pred_len,
        use_diff=train_cfg.use_diff,
        scaler_path=scaler_path,
    )
    print(f"  ç‰¹å¾: {feature_names}")
    print(f"  è®­ç»ƒé›†: {len(train_ds)} | éªŒè¯é›†: {len(val_ds)} | æµ‹è¯•é›†: {len(test_ds)}")

    # æµ‹è¯•æ¨¡å¼: åªç”¨å°‘é‡æ•°æ®
    if train_cfg.test_mode:
        from torch.utils.data import Subset
        n_test_samples = min(500, len(train_ds))
        train_ds = Subset(train_ds, range(n_test_samples))
        val_ds = Subset(val_ds, range(min(100, len(val_ds))))
        print(f"  [æµ‹è¯•æ¨¡å¼] ç¼©å‡ä¸º: è®­ç»ƒ {len(train_ds)} | éªŒè¯ {len(val_ds)}")

    train_loader = DataLoader(train_ds, batch_size=train_cfg.batch_size,
                              shuffle=True, num_workers=0, drop_last=True)
    val_loader = DataLoader(val_ds, batch_size=train_cfg.batch_size,
                            shuffle=False, num_workers=0, drop_last=False)

    # åˆ›å»ºæ¨¡å‹
    print("[2/4] æ„å»ºæ¨¡å‹...")
    model = Model(model_cfg).to(device)
    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  å¯è®­ç»ƒå‚æ•°: {param_count:,}")

    optimizer = torch.optim.AdamW(
        model.parameters(), lr=train_cfg.lr, weight_decay=train_cfg.weight_decay
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=train_cfg.epochs, eta_min=1e-6
    )
    criterion = nn.MSELoss()

    # è®­ç»ƒå¾ªç¯
    print("[3/4] å¼€å§‹è®­ç»ƒ...")
    best_val_loss = float('inf')
    patience_counter = 0
    train_losses = []
    val_losses = []

    for epoch in range(1, train_cfg.epochs + 1):
        # --- è®­ç»ƒ ---
        model.train()
        epoch_loss = 0.0
        n_batches = 0

        for x, y, last_vals in train_loader:
            x = x.to(device)       # [B, seq_len, n_features]
            y = y.to(device)       # [B, pred_len, n_features]

            optimizer.zero_grad()
            pred = model(x)        # [B, pred_len, n_features]
            loss = criterion(pred, y)
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            epoch_loss += loss.item()
            n_batches += 1

        avg_train_loss = epoch_loss / max(n_batches, 1)
        train_losses.append(avg_train_loss)

        # --- éªŒè¯ ---
        model.eval()
        val_loss = 0.0
        n_val = 0
        with torch.no_grad():
            for x, y, last_vals in val_loader:
                x = x.to(device)
                y = y.to(device)
                pred = model(x)
                loss = criterion(pred, y)
                val_loss += loss.item() * x.size(0)
                n_val += x.size(0)

        avg_val_loss = val_loss / max(n_val, 1)
        val_losses.append(avg_val_loss)

        scheduler.step()

        # æ—¥å¿—
        if epoch % max(1, train_cfg.epochs // 20) == 0 or epoch == 1:
            lr_now = optimizer.param_groups[0]['lr']
            print(f"  Epoch {epoch:4d}/{train_cfg.epochs} | "
                  f"Train Loss: {avg_train_loss:.6f} | "
                  f"Val Loss: {avg_val_loss:.6f} | "
                  f"LR: {lr_now:.2e}")

        # æ—©åœ
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': best_val_loss,
                'model_config': model_cfg.__dict__,
                'train_config': {
                    'use_diff': train_cfg.use_diff,
                    'pool_id': pool_id,
                    'feature_names': feature_names,
                },
            }
            torch.save(checkpoint, os.path.join(pool_dir, 'best_model.pt'))
        else:
            patience_counter += 1
            if patience_counter >= train_cfg.patience:
                print(f"  æ—©åœ: éªŒè¯æŸå¤± {train_cfg.patience} ä¸ª epoch æœªæ”¹å–„")
                break

    # ä¿å­˜è®­ç»ƒå†å²
    print("[4/4] ä¿å­˜ç»“æœ...")
    history = {'train_loss': train_losses, 'val_loss': val_losses}
    with open(os.path.join(pool_dir, 'train_history.json'), 'w') as f:
        json.dump(history, f, indent=2)

    # ä¿å­˜è®­ç»ƒæ›²çº¿
    try:
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt

        fig, ax = plt.subplots(1, 1, figsize=(10, 5))
        ax.plot(train_losses, label='Train Loss')
        ax.plot(val_losses, label='Val Loss')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('MSE Loss')
        ax.set_title(f'Pool {pool_id} Training Curve')
        ax.legend()
        ax.grid(True, alpha=0.3)
        fig.tight_layout()
        fig.savefig(os.path.join(pool_dir, 'train_curve.png'), dpi=150)
        plt.close(fig)
    except Exception as e:
        print(f"  [è­¦å‘Š] ç»˜å›¾å¤±è´¥: {e}")

    print(f"  âœ… æ± å­ {pool_id} è®­ç»ƒå®Œæˆ | æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"  æ¨¡å‹ä¿å­˜: {os.path.join(pool_dir, 'best_model.pt')}")

    return best_val_loss


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main():
    parser = argparse.ArgumentParser(description='xPatch å‡ºæ°´æµŠåº¦é¢„æµ‹è®­ç»ƒ')
    parser.add_argument('--csv', type=str, default='../train_data.csv',
                        help='CSV æ•°æ®è·¯å¾„')
    parser.add_argument('--pool', type=int, default=1, choices=[1, 2, 3, 4],
                        help='æ± å­ç¼–å·')
    parser.add_argument('--all', action='store_true', help='è®­ç»ƒæ‰€æœ‰æ± å­')
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--lr', type=float, default=1e-3)
    parser.add_argument('--seq-len', type=int, default=60)
    parser.add_argument('--pred-len', type=int, default=6)
    parser.add_argument('--patch-len', type=int, default=6)
    parser.add_argument('--stride', type=int, default=3)
    parser.add_argument('--ma-type', type=str, default='ema',
                        choices=['ema', 'dema', 'reg'])
    parser.add_argument('--alpha', type=float, default=0.5)
    parser.add_argument('--beta', type=float, default=0.5)
    parser.add_argument('--no-diff', action='store_true',
                        help='ä¸ä½¿ç”¨å·®åˆ†ç›®æ ‡ï¼ˆç›´æ¥é¢„æµ‹åŸå§‹å€¼ï¼‰')
    parser.add_argument('--no-revin', action='store_true',
                        help='ä¸ä½¿ç”¨ RevIN')
    parser.add_argument('--patience', type=int, default=10)
    parser.add_argument('--output', type=str, default='output')
    parser.add_argument('--test-mode', action='store_true',
                        help='CPU æµ‹è¯•æ¨¡å¼ï¼ˆå°‘é‡æ•°æ®å¿«é€ŸéªŒè¯ï¼‰')
    args = parser.parse_args()

    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    model_cfg = ModelConfig(
        seq_len=args.seq_len,
        pred_len=args.pred_len,
        enc_in=6,
        patch_len=args.patch_len,
        stride=args.stride,
        revin=not args.no_revin,
        ma_type=args.ma_type,
        alpha=args.alpha,
        beta=args.beta,
    )

    pools = [1, 2, 3, 4] if args.all else [args.pool]

    for pool_id in pools:
        train_cfg = TrainConfig(
            csv_path=args.csv,
            pool_id=pool_id,
            epochs=args.epochs,
            batch_size=args.batch_size,
            lr=args.lr,
            use_diff=not args.no_diff,
            patience=args.patience,
            output_dir=args.output,
            test_mode=args.test_mode,
        )
        train_one_pool(train_cfg, model_cfg, device)

    print("\nğŸ‰ å…¨éƒ¨è®­ç»ƒå®Œæˆï¼")


if __name__ == '__main__':
    main()
