"""
å‡ºæ°´æµŠåº¦é¢„æµ‹ - è¯„ä¼°ä¸å¯è§†åŒ–è„šæœ¬

åŠŸèƒ½:
  1. åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®— MAEã€RMSEã€RÂ²ã€æ–¹å‘å‡†ç¡®ç‡
  2. ä»æµ‹è¯•é›†ä¸­æŠ½æ ·è‹¥å¹²æ ·æœ¬ï¼Œç»˜åˆ¶:
     - 30 ç‚¹å†å²è¾“å…¥ï¼ˆturb_chushuiï¼‰
     - 6 ç‚¹é¢„æµ‹ vs å®é™…å¯¹æ¯”

ç”¨æ³•:
    python evaluate.py --pool 1
    python evaluate.py --pool 1 --n-samples 8   # æŠ½æ · 8 ä¸ªæ ·æœ¬ç»˜å›¾
    python evaluate.py --all                     # è¯„ä¼°æ‰€æœ‰æ± å­
"""

import os
import argparse
import json
import pickle
import numpy as np
import torch
from torch.utils.data import DataLoader
from dataclasses import dataclass

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

from dataset import (
    create_datasets, load_and_preprocess,
    TURB_CHUSHUI_IDX, POOL_FEATURES,
)
from models.xPatch import Model
from train import ModelConfig


# ============================================================
# æŒ‡æ ‡è®¡ç®—
# ============================================================

def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray):
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚

    Args:
        y_true, y_pred: shape [N, pred_len]

    Returns:
        dict åŒ…å«å„æ­¥å’Œæ€»ä½“æŒ‡æ ‡
    """
    mae = np.mean(np.abs(y_true - y_pred))
    mse = np.mean((y_true - y_pred) ** 2)
    rmse = np.sqrt(mse)

    # RÂ²
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    r2 = 1 - ss_res / (ss_tot + 1e-8)

    # æ–¹å‘å‡†ç¡®ç‡: é¢„æµ‹çš„å˜åŒ–æ–¹å‘ä¸å®é™…æ˜¯å¦ä¸€è‡´
    # è·Ÿä¸Šä¸€æ­¥æ¯”è¾ƒ (first step vs last input is implicitly 0 in diff mode)
    pred_diff = np.diff(y_pred, axis=1, prepend=0)
    true_diff = np.diff(y_true, axis=1, prepend=0)
    direction_acc = np.mean(np.sign(pred_diff) == np.sign(true_diff))

    # æŒ‰æ­¥æŒ‡æ ‡
    per_step = {}
    for step in range(y_true.shape[1]):
        step_mae = np.mean(np.abs(y_true[:, step] - y_pred[:, step]))
        step_rmse = np.sqrt(np.mean((y_true[:, step] - y_pred[:, step]) ** 2))
        per_step[f't+{step+1}'] = {'MAE': round(float(step_mae), 6),
                                     'RMSE': round(float(step_rmse), 6)}

    return {
        'MAE': round(float(mae), 6),
        'RMSE': round(float(rmse), 6),
        'R2': round(float(r2), 6),
        'Direction_Accuracy': round(float(direction_acc), 4),
        'per_step': per_step,
    }


# ============================================================
# åå·®åˆ† + åæ ‡å‡†åŒ–
# ============================================================

def inverse_transform_predictions(pred_scaled_diff, last_vals_scaled, scaler,
                                  use_diff=True):
    """
    å°†æ¨¡å‹è¾“å‡ºè½¬æ¢å›åŸå§‹ç©ºé—´ã€‚

    Args:
        pred_scaled_diff: [N, pred_len] æ¨¡å‹é¢„æµ‹çš„ turb_chushui é€šé“ï¼ˆscaled, å¯èƒ½æ˜¯å·®åˆ†ï¼‰
        last_vals_scaled: [N] æœ€åè¾“å…¥æ­¥çš„ turb_chushui å€¼ï¼ˆscaledï¼‰
        scaler: StandardScaler
        use_diff: æ˜¯å¦ä½¿ç”¨äº†å·®åˆ†

    Returns:
        pred_original: [N, pred_len] åŸå§‹ç©ºé—´çš„é¢„æµ‹å€¼
    """
    turb_idx = TURB_CHUSHUI_IDX
    n_features = scaler.n_features_in_

    if use_diff:
        # åå·®åˆ†: pred_abs_scaled = last_val + pred_diff
        pred_abs_scaled = pred_scaled_diff + last_vals_scaled[:, np.newaxis]
    else:
        pred_abs_scaled = pred_scaled_diff

    # åæ ‡å‡†åŒ– (åªå¯¹ turb_chushui é€šé“)
    mean_val = scaler.mean_[turb_idx]
    std_val = scaler.scale_[turb_idx]

    pred_original = pred_abs_scaled * std_val + mean_val
    return pred_original


def inverse_transform_values(values_scaled, scaler, feature_idx):
    """åæ ‡å‡†åŒ–å•ä¸ªç‰¹å¾"""
    mean_val = scaler.mean_[feature_idx]
    std_val = scaler.scale_[feature_idx]
    return values_scaled * std_val + mean_val


# ============================================================
# è¯„ä¼°å•ä¸ªæ± å­
# ============================================================

def evaluate_pool(pool_id: int, csv_path: str, output_dir: str,
                  n_samples: int = 6, device: torch.device = None):
    """è¯„ä¼°å•ä¸ªæ± å­ï¼Œè¾“å‡ºæŒ‡æ ‡å’Œå›¾è¡¨"""

    pool_dir = os.path.join(output_dir, f'pool_{pool_id}')
    ckpt_path = os.path.join(pool_dir, 'best_model.pt')
    scaler_path = os.path.join(pool_dir, 'scaler.pkl')

    if not os.path.exists(ckpt_path):
        print(f"  âŒ æ± å­ {pool_id}: æœªæ‰¾åˆ°æ¨¡å‹ {ckpt_path}")
        return

    print(f"\n{'='*60}")
    print(f"  è¯„ä¼°æ± å­ {pool_id}")
    print(f"{'='*60}")

    # åŠ è½½ checkpoint
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    model_cfg_dict = checkpoint['model_config']
    train_info = checkpoint['train_config']
    use_diff = train_info['use_diff']

    # é‡å»ºæ¨¡å‹é…ç½®
    model_cfg = ModelConfig(**model_cfg_dict)
    model = Model(model_cfg).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # åŠ è½½ scaler
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    _, _, test_ds, _, feature_names = create_datasets(
        csv_path=csv_path,
        pool_id=pool_id,
        seq_len=model_cfg.seq_len,
        pred_len=model_cfg.pred_len,
        use_diff=use_diff,
    )

    test_loader = DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=0)

    # æ”¶é›†é¢„æµ‹
    all_preds = []
    all_targets = []
    all_last_vals = []
    all_inputs = []

    turb_idx = TURB_CHUSHUI_IDX

    with torch.no_grad():
        for x, y, last_vals in test_loader:
            x = x.to(device)
            pred = model(x)  # [B, pred_len, n_features]

            # åªå– turb_chushui é€šé“
            pred_turb = pred[:, :, turb_idx].cpu().numpy()   # [B, pred_len]
            y_turb = y[:, :, turb_idx].numpy()               # [B, pred_len]
            lv = last_vals[:, turb_idx].numpy()               # [B]
            inp_turb = x[:, :, turb_idx].cpu().numpy()        # [B, seq_len]

            all_preds.append(pred_turb)
            all_targets.append(y_turb)
            all_last_vals.append(lv)
            all_inputs.append(inp_turb)

    all_preds = np.concatenate(all_preds, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    all_last_vals = np.concatenate(all_last_vals, axis=0)
    all_inputs = np.concatenate(all_inputs, axis=0)

    # è½¬å›åŸå§‹ç©ºé—´
    pred_original = inverse_transform_predictions(
        all_preds, all_last_vals, scaler, use_diff=use_diff
    )
    target_original = inverse_transform_predictions(
        all_targets, all_last_vals, scaler, use_diff=use_diff
    )
    input_original = inverse_transform_values(all_inputs, scaler, turb_idx)
    last_vals_original = inverse_transform_values(all_last_vals, scaler, turb_idx)

    # è®¡ç®—æŒ‡æ ‡
    metrics = compute_metrics(target_original, pred_original)
    print(f"\n  ğŸ“Š æ€»ä½“æŒ‡æ ‡:")
    print(f"     MAE:  {metrics['MAE']:.4f}")
    print(f"     RMSE: {metrics['RMSE']:.4f}")
    print(f"     RÂ²:   {metrics['R2']:.4f}")
    print(f"     æ–¹å‘å‡†ç¡®ç‡: {metrics['Direction_Accuracy']:.2%}")
    print(f"\n  ğŸ“Š åˆ†æ­¥æŒ‡æ ‡:")
    for step_name, step_metrics in metrics['per_step'].items():
        print(f"     {step_name}: MAE={step_metrics['MAE']:.4f}, RMSE={step_metrics['RMSE']:.4f}")

    # ä¿å­˜æŒ‡æ ‡
    fig_dir = os.path.join(pool_dir, 'figures')
    os.makedirs(fig_dir, exist_ok=True)
    with open(os.path.join(pool_dir, 'test_metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)

    # ---- å¯è§†åŒ– ----

    # 1. åˆ†æ­¥ MAE/RMSE æŸ±çŠ¶å›¾
    _plot_per_step_metrics(metrics, pool_id, fig_dir)

    # 2. æŠ½æ ·æ ·æœ¬: 30 ç‚¹å†å² + 6 ç‚¹é¢„æµ‹ vs å®é™…
    _plot_sample_predictions(
        input_original, pred_original, target_original,
        pool_id, n_samples, fig_dir,
        seq_len=model_cfg.seq_len, pred_len=model_cfg.pred_len,
    )

    # 3. å…¨æµ‹è¯•é›†æ•£ç‚¹å›¾
    _plot_scatter(target_original, pred_original, pool_id, fig_dir)

    print(f"\n  ğŸ“ å›¾è¡¨å·²ä¿å­˜è‡³: {fig_dir}")
    return metrics


# ============================================================
# å¯è§†åŒ–å‡½æ•°
# ============================================================

def _plot_per_step_metrics(metrics, pool_id, fig_dir):
    """åˆ†æ­¥ MAE/RMSE æŸ±çŠ¶å›¾"""
    steps = list(metrics['per_step'].keys())
    maes = [metrics['per_step'][s]['MAE'] for s in steps]
    rmses = [metrics['per_step'][s]['RMSE'] for s in steps]

    fig, ax = plt.subplots(figsize=(8, 4))
    x_pos = np.arange(len(steps))
    width = 0.35
    ax.bar(x_pos - width/2, maes, width, label='MAE', color='#4ecdc4')
    ax.bar(x_pos + width/2, rmses, width, label='RMSE', color='#ff6b6b')
    ax.set_xticks(x_pos)
    ax.set_xticklabels(steps)
    ax.set_ylabel('Error')
    ax.set_title(f'Pool {pool_id} â€” Per-Step Errors')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    fig.tight_layout()
    fig.savefig(os.path.join(fig_dir, 'per_step_errors.png'), dpi=150)
    plt.close(fig)


def _plot_sample_predictions(input_orig, pred_orig, target_orig,
                             pool_id, n_samples, fig_dir,
                             seq_len=30, pred_len=6):
    """
    ä»æµ‹è¯•é›†ä¸­æŠ½æ · n_samples ä¸ªä¾‹å­ï¼Œæ¯ä¸ªä¾‹å­ç”»:
    - 30 ç‚¹å†å²è¾“å…¥ (turb_chushui)
    - 6 ç‚¹é¢„æµ‹ vs 6 ç‚¹å®é™…
    """
    total = len(input_orig)
    # å‡åŒ€æŠ½æ ·
    indices = np.linspace(0, total - 1, n_samples, dtype=int)

    n_cols = min(3, n_samples)
    n_rows = (n_samples + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4 * n_rows))
    if n_samples == 1:
        axes = np.array([axes])
    axes = axes.flatten()

    for i, idx in enumerate(indices):
        ax = axes[i]

        # å†å²éƒ¨åˆ†
        hist = input_orig[idx]          # [seq_len]
        pred = pred_orig[idx]           # [pred_len]
        actual = target_orig[idx]       # [pred_len]

        t_hist = np.arange(seq_len)
        t_pred = np.arange(seq_len, seq_len + pred_len)

        ax.plot(t_hist, hist, 'b-o', markersize=2, linewidth=1.2,
                label='History', alpha=0.8)
        ax.plot(t_pred, actual, 'g-s', markersize=4, linewidth=1.5,
                label='Actual', alpha=0.9)
        ax.plot(t_pred, pred, 'r-^', markersize=4, linewidth=1.5,
                label='Predicted', alpha=0.9)

        # è¿æ¥çº¿: æœ€åå†å²ç‚¹ â†’ ç¬¬ä¸€ä¸ªé¢„æµ‹/å®é™…ç‚¹
        ax.plot([t_hist[-1], t_pred[0]], [hist[-1], actual[0]],
                'g--', alpha=0.4, linewidth=0.8)
        ax.plot([t_hist[-1], t_pred[0]], [hist[-1], pred[0]],
                'r--', alpha=0.4, linewidth=0.8)

        # é¢„æµ‹åŒºåŸŸèƒŒæ™¯è‰²
        ax.axvspan(seq_len - 0.5, seq_len + pred_len - 0.5,
                   alpha=0.08, color='orange')

        ax.set_title(f'Sample #{idx}', fontsize=10)
        ax.set_xlabel('Time Step (5min)')
        ax.set_ylabel('turb_chushui')
        ax.legend(fontsize=7, loc='upper left')
        ax.grid(True, alpha=0.2)

    # éšè—å¤šä½™å­å›¾
    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)

    fig.suptitle(f'Pool {pool_id} â€” Sample Predictions (30-in â†’ 6-out)',
                 fontsize=14, fontweight='bold')
    fig.tight_layout(rect=[0, 0, 1, 0.95])
    fig.savefig(os.path.join(fig_dir, 'sample_predictions.png'), dpi=150)
    plt.close(fig)
    print(f"  ğŸ“ˆ æŠ½æ ·é¢„æµ‹å›¾å·²ä¿å­˜ ({n_samples} ä¸ªæ ·æœ¬)")


def _plot_scatter(target_orig, pred_orig, pool_id, fig_dir):
    """å…¨æµ‹è¯•é›†é¢„æµ‹ vs å®é™…æ•£ç‚¹å›¾"""
    fig, ax = plt.subplots(figsize=(6, 6))

    y_flat = target_orig.flatten()
    p_flat = pred_orig.flatten()

    ax.scatter(y_flat, p_flat, alpha=0.05, s=2, c='#2196f3')

    # å¯¹è§’çº¿
    mn = min(y_flat.min(), p_flat.min())
    mx = max(y_flat.max(), p_flat.max())
    ax.plot([mn, mx], [mn, mx], 'r--', linewidth=1, label='y=x')

    ax.set_xlabel('Actual')
    ax.set_ylabel('Predicted')
    ax.set_title(f'Pool {pool_id} â€” Predicted vs Actual')
    ax.legend()
    ax.grid(True, alpha=0.2)
    ax.set_aspect('equal', adjustable='box')
    fig.tight_layout()
    fig.savefig(os.path.join(fig_dir, 'scatter_pred_vs_actual.png'), dpi=150)
    plt.close(fig)


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main():
    parser = argparse.ArgumentParser(description='xPatch å‡ºæ°´æµŠåº¦è¯„ä¼°')
    parser.add_argument('--csv', type=str, default='../train_data.csv')
    parser.add_argument('--pool', type=int, default=1, choices=[1, 2, 3, 4])
    parser.add_argument('--all', action='store_true', help='è¯„ä¼°æ‰€æœ‰æ± å­')
    parser.add_argument('--output', type=str, default='output')
    parser.add_argument('--n-samples', type=int, default=6,
                        help='æŠ½æ ·ç»˜å›¾çš„æ ·æœ¬æ•°')
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    pools = [1, 2, 3, 4] if args.all else [args.pool]

    for pool_id in pools:
        evaluate_pool(
            pool_id=pool_id,
            csv_path=args.csv,
            output_dir=args.output,
            n_samples=args.n_samples,
            device=device,
        )

    print("\nğŸ‰ è¯„ä¼°å®Œæˆï¼")


if __name__ == '__main__':
    main()
